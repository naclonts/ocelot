# Scenario Generator — Design Document

**Phase 2 Step 6 of `docs/PHASE2_PLAN.md`**

Goal: generate randomized simulation episodes for VLA training data collection.
Each episode is a fully-specified `ScenarioConfig` that deterministically reproduces
the Gazebo scene, face motion, and language label from a single integer seed.

---

## 1. New Files

| File | Purpose |
|---|---|
| `sim/worlds/scenario_world.sdf` | Clean world — no static face/background; everything spawned per-episode |
| `sim/assets/backgrounds/` | Background texture library (DVC-tracked alongside faces) |
| `sim/scenario_generator/scenario.py` | `ScenarioConfig` dataclass + `ScenarioGenerator.sample()` |
| `sim/scenario_generator/labels.py` | Multi-tier language label registry + `assign_label()` |
| `sim/scenario_generator/motion.py` | Four motion pattern classes |
| `sim/scenario_generator/gazebo_bridge.py` | gz.transport spawn/despawn/set_pose/spawn_light helpers |
| `sim/scenario_generator/episode_runner.py` | Episode orchestration (spawn → run → teardown) |
| `tests/sim/test_scenario_generator.py` | Pure-Python unit tests (no ROS/Gazebo required) |

## 2. Modified Files

| File | Change |
|---|---|
| `sim/models/face_billboard/model.sdf` | Add `<transparency>1</transparency>` for alpha blending |
| `launch/sim_launch.py` | Add `world` launch arg (`tracker_world` default, `scenario_world` for collection) |
| `sim/assets.dvc` | Will be regenerated after adding background textures to `sim/assets/` |
| `Makefile` | Add `backgrounds` target alongside existing `faces` target |

---

## 3. Asset Library

### 3a — Face textures (existing)

100 PNGs in `sim/assets/faces/`, DVC-tracked via `sim/assets.dvc`.
Attributes stored in `sim/scenario_generator/face_descriptions*.json` — **git-tracked**,
not DVC. Small text metadata (~50 KB per batch) tightly coupled to the scenario
generator code; must be present after `git clone` without requiring `dvc pull`.

File naming convention: `face_descriptions.json` for the canonical first batch (seed 7,
count 100); `face_descriptions_002.json`, `face_descriptions_003.json` etc. for
subsequent batches with different seeds/counts.

`ScenarioGenerator` globs `sim/scenario_generator/face_descriptions*.json` at
construction and merges all batches into one face pool. Adding a new batch is
just adding a new JSON file — no code change required.

Generated by: `python3 sim/scenario_generator/face_descriptions.py --count 100 --seed 7`

### 3b — Background textures (new)

Target: ~20 diverse background images.

```
sim/assets/backgrounds/
  indoor_office.jpg
  indoor_kitchen.jpg
  indoor_living_room.jpg
  outdoor_park.jpg
  outdoor_street.jpg
  outdoor_garden.jpg
  abstract_gradient_blue.jpg
  abstract_gradient_warm.jpg
  plain_white.jpg
  plain_gray_light.jpg
  plain_gray_dark.jpg
  plain_beige.jpg
  ...
```

**Breakdown (~20 total):**
- ~6 plain solid colors (white, light gray, dark gray, beige, off-white, blue-gray) —
  generated programmatically as 512×512 solid PNGs via `generate_backgrounds.py`;
  no external API needed (`np.full((512, 512, 3), rgb)`).
  Rationale: prevents the VLA from using background texture as a tracking cue; forces
  reliance on the face itself. Approximates real-world deployment (plain walls).
  Target ~30% of episodes to use a plain background.
- ~7 outdoor photo textures (park, street, garden, forest, beach, cityscape, field)
- ~7 indoor photo textures (office, kitchen, living room, library, gym, hallway, café)

Acquisition: freely-licensed stock photos (Unsplash, Pexels) or generated via the
same gpt-image-1 pipeline used for faces.  Add a `backgrounds_manifest.json`
alongside them with `{"id": "indoor_office", "tags": ["indoor", "busy"]}` per entry —
tags are used by the scenario generator to prefer diverse sampling.  Plain colors get
tag `"plain"`; the scenario generator samples plain vs. textured at a 30/70 split.

Add to DVC: `dvc add sim/assets` (same pointer file — adding more files to an
already-tracked directory just regenerates the `.dvc` checksum).

**Makefile target:**
```makefile
backgrounds:
	python3 sim/scenario_generator/generate_backgrounds.py \
	  --out sim/assets/backgrounds/
```

---

## 4. Scenario World File: `sim/worlds/scenario_world.sdf`

A minimal world that contains only the fixed infrastructure:

- Physics + core plugins (same as `tracker_world.sdf`)
- Ground plane (static, no texture — the camera never looks at the floor)
- **No** static face billboard
- **No** static background wall
- **No** directional sun light

Everything else is spawned per-episode by `gazebo_bridge.py`.

The `launch/sim_launch.py` already resolves the world file from a package path.
Add a `world` launch arg:
```
ros2 launch ocelot sim_launch.py world:=scenario_world headless:=true use_oracle:=true
```
Default remains `tracker_world` so existing Step 4/5 workflows are unchanged.

---

## 5. Scenario Configuration Schema

### 5a — Dataclasses (`sim/scenario_generator/scenario.py`)

```python
@dataclass
class FaceConfig:
    face_id:      str        # "face_042" — stem of PNG in sim/assets/faces/
    texture_path: str        # absolute path, resolved at sample time
    # Attributes are NOT duplicated here — look them up via face_id in
    # face_descriptions_001.json.  ScenarioGenerator loads that JSON once at
    # construction and uses it during sample() to compute the label.
    # The resulting label_key + language_label are stored on ScenarioConfig.
    initial_x:    float      # distance in front of robot [1.0–3.0 m]
    initial_y:    float      # lateral offset [-1.0–1.0 m]
    initial_z:    float      # height [0.5–1.5 m]
    motion:       str        # static | linear_drift | sinusoidal | random_walk
    speed:        float      # m/s [0.05–0.5] — peak velocity for all patterns
    period:       float      # seconds [6.0–20.0] — sinusoidal only; ignored otherwise

@dataclass
class DistractorConfig:
    shape:     str           # sphere | box
    color_rgb: tuple[float, float, float]   # each channel [0.2–0.9]
    initial_x: float
    initial_y: float
    initial_z: float
    speed:     float         # [0.02–0.2 m/s]

@dataclass
class ScenarioConfig:
    scenario_id:              str    # sha1[:8] of all params (for dedup + split)
    seed:                     int
    faces:                    list[FaceConfig]   # length 1–3
    target_face_idx:          int    # index into faces[] — oracle tracks this one
    background_id:            str    # stem of background image
    background_path:          str    # absolute path
    lighting_azimuth_deg:     float  # [0–360]
    lighting_elevation_deg:   float  # [15–75]
    lighting_intensity:       float  # [0.5–2.0] — point light intensity multiplier
    ambient_rgb:              tuple[float, float, float]  # scene ambient [0.2–0.8]
    distractor_count:         int    # [0–2]
    distractors:              list[DistractorConfig]
    camera_noise_sigma:       float  # [0.0–0.015]
    camera_brightness_offset: float  # [-20–+20] pixel value
    label_key:                str    # semantic category e.g. "hat_target"
    language_label:           str    # actual string e.g. "track the person in the hat"
```

### 5b — Randomization parameter ranges

| Parameter | Range | Notes |
|---|---|---|
| `faces` count | 1–3 | ~60% single, ~30% two, ~10% three |
| `initial_x` | 1.0–3.0 m | |
| `initial_y` | −1.0–1.0 m | |
| `initial_z` | 0.5–1.5 m | |
| `motion` | static / linear_drift / sinusoidal / random_walk | ~30% static for single-face episodes (prevents "track motion" shortcut) |
| `speed` | 0.05–0.5 m/s | peak velocity; each motion class interprets this for its pattern |
| `period` | 6.0–20.0 s | sinusoidal only; derived amplitude = speed × period / (2π) |
| `background_id` | uniform sample from library | |
| `lighting_azimuth_deg` | 0–360° | |
| `lighting_elevation_deg` | 15–75° | |
| `lighting_intensity` | 0.5–2.0 | |
| `ambient_rgb` | [0.2, 0.8] per channel | sampled independently |
| `distractor_count` | 0–2 | |
| `camera_noise_sigma` | 0.0–0.015 | applied in collect_data.py |
| `camera_brightness_offset` | −20–+20 | applied in collect_data.py |

### 5c — `ScenarioGenerator.sample(seed)`

```python
class ScenarioGenerator:
    def __init__(self, faces_dir: Path, backgrounds_dir: Path):
        # Glob all face_descriptions*.json in faces_dir, merge into one pool.
        # This means adding a new batch is just dropping a new JSON file — no
        # code change required.
        self._faces = []
        for json_path in sorted(faces_dir.glob("face_descriptions*.json")):
            self._faces.extend(json.loads(json_path.read_text()))
        ...

    def sample(self, seed: int) -> ScenarioConfig:
        rng = random.Random(seed)
        # sample all parameters
        # call labels.assign_label(faces, target_idx, face_attributes, rng)
        # build scenario_id from sha1 of all params
        ...
```

---

## 6. Language Label System (`sim/scenario_generator/labels.py`)

### 6a — Multi-tier label registry

Each semantic condition maps to a bucket of label templates across three tiers.
At generation time one template is chosen at random from the combined bucket.

```python
LABEL_REGISTRY: dict[str, list[str]] = {

    # ── Single-face conditions ────────────────────────────────────────────────

    "single_centered": [
        # full
        "track the person in the center",
        "follow the face in front of you",
        # short
        "track the face",
        "follow them",
        "keep them centered",
        # descriptive
        "look at the person in front of you",
        "keep the face in frame",
    ],

    "single_slow": [
        "follow slowly",
        "track gently",
        "go slow",
        "take it slow following the face",
        "follow that person, but slowly",
        "keep up with them, no rush",
    ],

    "single_left": [
        # full
        "track the face on the left",
        "follow the person to the left",
        # short
        "look left",
        "track left",
        # descriptive
        "the face is on the left — keep it centered",
        "follow whoever is to your left",
    ],

    "single_right": [
        "track the face on the right",
        "follow the person to the right",
        "look right",
        "track right",
        "the face is on the right — keep it centered",
        "follow whoever is to your right",
    ],

    # ── Multi-face: distinguishing attribute conditions ───────────────────────
    # These are parameterised — {attr} is substituted at generation time.
    # attr examples: "hat", "glasses", "beard", "sunglasses", "baseball cap"

    "multi_attr": [
        # full
        "track the person wearing the {attr}",
        "follow the one with the {attr}",
        # short
        "track the {attr}",
        "follow the {attr}",
        # descriptive
        "look at the one with the {attr} on",
        "keep your eye on the person with the {attr}",
        "find the {attr} and follow them",
    ],

    # ── Multi-face: positional conditions ────────────────────────────────────

    "multi_left": [
        "follow the person on the left",
        "track the leftmost person",
        "look to the left one",
        "keep your eye on the person furthest left",
        "the one on the left — follow them",
    ],

    "multi_right": [
        "follow the person on the right",
        "track the rightmost person",
        "look to the right one",
        "keep your eye on the person furthest right",
        "the one on the right — follow them",
    ],

    # ── Multi-face: proximity condition ──────────────────────────────────────

    "multi_closest": [
        "track the closest person",
        "follow the nearest one",
        "keep the person closest to you centered",
        "track whoever is nearest",
        "focus on the closest face",
        "stay with the one right in front of you",
    ],
}
```

### 6b — `assign_label(faces, target_idx, face_attrs, rng)`

Priority order when selecting a condition (first match wins):

1. **`multi_attr`** — any face pair where target has a hat/glasses/beard that no
   other face has. Compute the distinguishing attr string and substitute `{attr}`.
2. **`multi_left` / `multi_right`** — multi-face, target is leftmost/rightmost by `initial_y`.
3. **`multi_closest`** — multi-face, target has smallest `initial_x` (closest to camera).
4. **`single_slow`** — single face with `speed < 0.15 m/s` AND `motion != static`.
5. **`single_left` / `single_right`** — single face with `|initial_y| > 0.3 m`.
6. **`single_centered`** — fallback for all remaining single-face cases.
   Also used for static-face episodes regardless of position (the face isn't going
   anywhere so left/right is less meaningful).

Return value: `(label_key, language_label)` — both stored in `ScenarioConfig`.

### 6c — Distinguishable attribute resolution

For condition `multi_attr`, scan face attributes from `face_descriptions_{num}.json`. Combine any json files matching that pattern.
Attribute priority for distinguishability (pick first that applies to target only):

1. Hat (`hat` field is non-null for target, null for all others)
2. Sunglasses (`glasses == "sunglasses"` for target, others don't have sunglasses)
3. Any glasses (`glasses` non-null for target, null for all others)
4. Beard (`facial_hair` non-null for target, null for all others)
5. Accessory (`accessory` non-null for target, null for all others)

Displayable attr strings:
```python
ATTR_DISPLAY = {
    "baseball_cap": "baseball cap",
    "beanie": "beanie",
    "fedora": "fedora",
    "wide_brim": "sun hat",
    "pirate_hat": "pirate hat",
    "cowboy_hat": "cowboy hat",
    "reading": "glasses",
    "round": "round glasses",
    "rectangular": "glasses",
    "thick_rimmed": "thick-rimmed glasses",
    "sunglasses": "sunglasses",
    "stubble": "stubble",
    "beard": "beard",
    "mustache": "mustache",
    "goatee": "goatee",
    "over_ear_headphones": "headphones",
    "scarf": "scarf",
}
```

---

## 7. Motion Patterns (`sim/scenario_generator/motion.py`)

All patterns share the interface:

```python
class MotionPattern:
    def reset(self, x0, y0, z0): ...
    def step(self, t: float) -> tuple[float, float, float]:
        """Return absolute world position at time t seconds."""
```

### `StaticMotion`
Always returns the initial position. Used for ~30% of single-face episodes.

### `SinusoidalMotion(speed, period_y)`
```
amp_y   = speed * period_y / (2π)         # peak velocity = speed
period_z = period_y * 1.7                 # incommensurate (no path repeat)
amp_z   = amp_y * 0.4                     # tilt oscillation smaller than pan

y(t) = clip(y0 + amp_y * sin(2π t / period_y),  -1.0, 1.0)
z(t) = clip(z0 + amp_z * sin(2π t / period_z),   0.3, 1.7)
x(t) = x0  (constant depth)
```
`period_y` is sampled per episode from 6–20 s, so slow sinusoids have large
swings and fast sinusoids have small tight swings — both at the same `speed`.

### `LinearDriftMotion(vy, vz)`
```
y(t) = clip(y0 + vy * t, -1.0, 1.0)
z(t) = clip(z0 + vz * t,  0.3, 1.7)
```
Reverses direction when hitting a world boundary.

### `RandomWalkMotion(speed, tau=3.0)`
Ornstein-Uhlenbeck process on velocity: `v += -v/τ * dt + noise * sqrt(2/τ) * dW`.
Clamps position to the same bounds as LinearDrift.
Seeded from the scenario seed for reproducibility.

---

## 8. Gazebo Bridge (`sim/scenario_generator/gazebo_bridge.py`)

Single persistent `gz.transport13.Node()` for the process lifetime (avoids
repeated ZMQ teardown, as proven in `sim/move_face.py`).

### Entity naming convention

| Entity | Gazebo name |
|---|---|
| Primary target face | `face_0` |
| Second face | `face_1` |
| Third face | `face_2` |
| Distractor 0 | `distractor_0` |
| Distractor 1 | `distractor_1` |
| Key light | `episode_light_key` |
| Fill light (optional) | `episode_light_fill` |
| Background wall | `background_wall` |

All episode-scoped entities are despawned at episode end.

### API

```python
WORLD = "tracker_world"    # or "scenario_world"

def spawn_face(name: str, pos: tuple, texture_abs_path: str) -> bool:
    """Spawn a face billboard with the given texture via /world/{world}/create."""
    # Inline SDF: thin box, PBR albedo_map = texture_abs_path,
    # <transparency>1</transparency> for alpha blending,
    # PosePublisher plugin (only on face_0 — oracle only tracks primary target)

def despawn(name: str) -> bool:
    """Remove a named entity via /world/{world}/remove."""

def set_pose(name: str, x: float, y: float, z: float) -> bool:
    """Set absolute world position. Rotation always identity."""

def spawn_background(background_abs_path: str) -> bool:
    """Despawn previous background_wall and spawn a new one with the given texture.
    Geometry: same as model.sdf — 0.002 × 120.0 × 40.0 m box at x=50, z=20."""

def spawn_key_light(azimuth_deg: float, elevation_deg: float, intensity: float) -> bool:
def spawn_fill_light(azimuth_deg: float, elevation_deg: float, ambient_rgb: tuple) -> bool:
    """Soft fill light at azimuth+180°, same elevation.
    diffuse = ambient_rgb; intensity fixed at 0.6 (always softer than key)."""
    """Spawn a point light at a position on a hemisphere above the scene.
    Spherical → Cartesian:
        r = 6.0 m from scene origin
        x = r * cos(el) * cos(az) + 1.5   (offset to be near scene centre)
        y = r * cos(el) * sin(az)
        z = r * sin(el)
    Attenuation: constant=0.1, linear=0.01, quadratic=0.001
    Range: 20 m."""

def spawn_distractor(name: str, pos: tuple,
                     shape: str, color_rgb: tuple) -> bool:
    """Spawn a colored sphere or box primitive.
    No PosePublisher — oracle must not see it as a trackable target."""

def setup_episode(config: ScenarioConfig) -> None:
    """Teardown any previous episode entities, then spawn everything for config."""

def teardown_episode() -> None:
    """Despawn all episode-scoped entities."""
```

### Inline SDF template for face billboard

```xml
<sdf version="1.10">
  <model name="{name}">
    <static>true</static>
    {pose_publisher_plugin}
    <link name="link">
      <visual name="face_visual">
        <transparency>1</transparency>
        <geometry>
          <box><size>0.002 0.5 0.5</size></box>
        </geometry>
        <material>
          <ambient>1 1 1 1</ambient>
          <diffuse>1 1 1 1</diffuse>
          <pbr><metal>
            <albedo_map>{texture_abs_path}</albedo_map>
            <metalness>0.0</metalness>
            <roughness>1.0</roughness>
          </metal></pbr>
        </material>
      </visual>
    </link>
  </model>
</sdf>
```

`{pose_publisher_plugin}` is only included for `face_0` (the oracle target).
Billboard size is 0.5 × 0.5 m to accommodate waist-up crops.

### Inline SDF template for background wall

Identical to `sim/models/background_wall/model.sdf` but with `albedo_map` replaced
by the absolute path to the chosen background texture.

### Inline SDF template for point light

```xml
<sdf version="1.10">
  <light type="point" name="{name}">
    <pose>{x} {y} {z} 0 0 0</pose>
    <diffuse>{r} {g} {b} 1</diffuse>
    <specular>0.1 0.1 0.1 1</specular>
    <attenuation>
      <range>20</range>
      <constant>0.1</constant>
      <linear>0.01</linear>
      <quadratic>0.001</quadratic>
    </attenuation>
    <cast_shadows>false</cast_shadows>
  </light>
</sdf>
```

---

## 9. Episode Runner (`sim/scenario_generator/episode_runner.py`)

```python
class EpisodeRunner:
    def __init__(self, bridge: GazeboBridge): ...

    def setup(self, config: ScenarioConfig) -> None:
        """Spawn all entities for this episode."""
        bridge.setup_episode(config)
        # initialise a MotionPattern per face and per distractor

    def step(self, t: float) -> dict[str, tuple]:
        """Advance all motion patterns to time t.
        Returns {entity_name: (x, y, z)} for all dynamic entities.
        Calls bridge.set_pose for each."""

    def teardown(self) -> None:
        bridge.teardown_episode()
```

`collect_data.py` (Step 7) calls `setup()`, then ticks `step(t)` at 10 Hz in sync
with the camera topic, records frames + oracle actions, then calls `teardown()`.

The episode runner does **not** touch ROS. It only speaks gz.transport.

---

## 10. Camera Noise and Brightness (applied in `collect_data.py`)

Applied as NumPy post-processing in `collect_data.py` at frame capture time,
not in Gazebo. This works correctly for VLA training because:

1. **The VLA trains on stored frames** — the HDF5 files contain post-processed frames.
   The VLA never sees raw Gazebo output; noise baked in at collection time is
   indistinguishable from having been rendered that way.
2. **Oracle is decoupled from the camera** — the oracle tracks `/model/face_0/pose`
   (ground-truth 3D position), not the camera image. Noise doesn't degrade label
   quality; it only makes the training distribution harder, which is the point.
3. **More flexible than Gazebo sensor noise** — the sensor plugin has limited options;
   NumPy lets you tune distribution, magnitude, and seed precisely.


```python
frame = np.array(msg)                                    # uint8 H×W×3
if config.camera_noise_sigma > 0:
    noise = rng.normal(0, config.camera_noise_sigma * 255, frame.shape)
    frame = np.clip(frame + noise, 0, 255).astype(np.uint8)
frame = np.clip(frame.astype(int) + config.camera_brightness_offset, 0, 255).astype(np.uint8)
```

---

## 11. Run Summaries (git-tracked)

Per-episode `ScenarioConfig` files are **not** saved standalone — they live as
`metadata` JSON strings inside each HDF5 episode file (Step 7). Reproducibility
comes from the seed.

However, after each named collection run, `collect_data.py` writes a small
**run summary** to `sim/scenario_generator/runs/<run_name>.json` which is
committed to git. This gives a human-readable, diffable record of each dataset
version without bloating the repo.

```
sim/scenario_generator/runs/
  train_v1.json
  train_v2.json
  ...
```

Each run summary contains:
```json
{
  "run_name": "train_v1",
  "date": "2026-02-24",
  "n_episodes": 10000,
  "seeds": [0, 9999],
  "label_distribution": {"single_centered": 0.31, "single_slow": 0.08, ...},
  "face_count_distribution": {"1": 0.61, "2": 0.28, "3": 0.11},
  "motion_distribution": {"static": 0.29, "sinusoidal": 0.26, ...},
  "background_distribution": {"plain": 0.31, "indoor": 0.35, "outdoor": 0.34},
  "param_ranges": {
    "speed": [0.05, 0.49],
    "lighting_elevation_deg": [15.2, 74.8]
  }
}
```

`collect_data.py` generates this automatically at the end of each run.
The `runs/` directory is git-tracked (not DVC) — it's tiny and highly reviewable.

---

## 12. Tests (`tests/sim/test_scenario_generator.py`)

All tests are pure Python — no ROS, no Gazebo, no network required.

| Test | What it asserts |
|---|---|
| `test_label_determinism` | Same seed → same `language_label` across 10 re-samples |
| `test_label_correctness_hat` | Scenario where only target has a hat → `multi_attr` condition, label contains "hat" |
| `test_label_correctness_slow` | Single face, speed=0.1, motion=sinusoidal → `single_slow` condition |
| `test_label_correctness_left` | Single face, initial_y=-0.6 → `single_left` condition |
| `test_label_coverage` | Sample 500 scenarios, assert all 6 label keys appear |
| `test_bounds_check` | Sample 1000 configs, assert every numeric param is within declared range |
| `test_face_count_distribution` | Sample 1000 configs, single≈60%, two≈30%, three≈10% ± 5% |
| `test_motion_static_fraction` | Single-face episodes: static motion ≈ 30% ± 5% |
| `test_scenario_id_uniqueness` | Sample 500 configs with different seeds, no repeated `scenario_id` |
| `test_json_round_trip` | `ScenarioConfig → dict → JSON → dict → ScenarioConfig` is lossless |
| `test_motion_patterns` | All 4 motion patterns return positions within world bounds after 60 s |
| `test_multi_attr_priority` | Hat takes priority over glasses when target has both and no other face has either |

---

## 13. Implementation Steps (for agent handoff)

Each step is self-contained and can be handed to a fresh agent.
Reference this document and `docs/PHASE2_PLAN.md` as context.

### Step A — `scenario_world.sdf` + launch arg ✅ DONE

**Files**: `sim/worlds/scenario_world.sdf`, `launch/sim_launch.py`

Create a minimal world SDF with only:
- Physics + UserCommands + SceneBroadcaster + Sensors plugins
- Ground plane (no texture)
- `<scene>` with low ambient (0.15 0.15 0.15 1) — episode light provides the main illumination
- No face, no background, no sun light

Add `world` arg to `sim_launch.py` (default `tracker_world`).

**Acceptance**: `ros2 launch ocelot sim_launch.py world:=scenario_world headless:=true`
starts cleanly; `gz model --list` shows only `ground_plane` and `ocelot`; no errors.

---

### Step B — Background texture library [PARTIAL]

**Files**: `sim/assets/backgrounds/*.jpg`, `sim/assets/backgrounds/backgrounds_manifest.json`,
`sim/scenario_generator/generate_backgrounds.py`, `Makefile`

Acquire ~20 background textures. Recommended sources: Unsplash (free license).
Categories: indoor (≥5), outdoor (≥5), abstract/plain (≥5), mixed (remainder).

`backgrounds_manifest.json`:
```json
[
  {"id": "indoor_office", "tags": ["indoor", "busy"], "file": "indoor_office.jpg"},
  ...
]
```

Update DVC: backgrounds live in `sim/assets/backgrounds/`, same DVC-tracked tree.
Run `dvc add sim/assets` to regenerate `sim/assets.dvc`.

Add `Makefile` target `backgrounds` that documents how the textures were sourced
(even if that's just a shell script that downloads them).

**Status**:
- ✅ `generate_backgrounds.py` script written (generates 6 plain-color PNGs)
- ✅ `Makefile` `backgrounds` target added
- ❌ Script not yet run — `sim/assets/backgrounds/` does not exist, no PNGs generated
- ❌ Photo textures (indoor/outdoor, ~14 images) not yet acquired
- ❌ DVC not updated (`dvc add sim/assets` not run)

**Acceptance**: `ls sim/assets/backgrounds/ | wc -l` ≥ 20, `dvc status` is clean.

---

### Step C — `ScenarioConfig` dataclass + `ScenarioGenerator` ✅ DONE

**Files**: `sim/scenario_generator/scenario.py`

Implement all dataclasses from §5a, `ScenarioGenerator.sample(seed)`, and
`ScenarioConfig.to_dict()` / `from_dict()` for JSON serialization.

Does NOT implement label assignment (that's Step D) — use a placeholder
`language_label = ""` and `label_key = ""` until Step D is merged.

**Acceptance**: `python3 -c "from sim.scenario_generator.scenario import ScenarioGenerator; ..."`:
- 1000 sampled configs, all numeric params within range
- JSON round-trip lossless
- `scenario_id` unique across 500 seeds

---

### Step D — Language label system ✅ DONE

**Files**: `sim/scenario_generator/labels.py`, update `sim/scenario_generator/scenario.py`
to call `assign_label()`.

Implement `LABEL_REGISTRY`, `assign_label(faces, target_idx, face_attrs_list, rng)`,
and `ATTR_DISPLAY`. Wire into `ScenarioGenerator.sample()`.

**Acceptance**: all label tests in §11 pass.

---

### Step E — Motion patterns ✅ DONE

**Files**: `sim/scenario_generator/motion.py`

Implement the four pattern classes from §7. Each must:
- Accept an RNG seed at construction for reproducibility
- Implement `reset(x0, y0, z0)` and `step(t) -> (x, y, z)`
- Keep positions within world bounds (x: 1.0–3.0, y: −1.0–1.0, z: 0.3–1.7)

**Acceptance**: motion pattern tests in §11 pass.

---

### Step F — Gazebo bridge ✅ DONE (fill light added)

**Files**: `sim/scenario_generator/gazebo_bridge.py`

Implement all functions from §8. Key implementation notes:

- Use `gz.transport13` (same version as `move_face.py`)
- Single persistent `Node()` at module level (not per-call)
- `spawn_face` / `spawn_background` / `spawn_key_light` / `spawn_distractor` all
  use `/world/{WORLD}/create` with inline SDF strings
- `despawn` uses `/world/{WORLD}/remove`
- `set_pose` uses `/world/{WORLD}/set_pose` (proven in `move_face.py`)
- `setup_episode` calls `teardown_episode` first, then spawns fresh
- `teardown_episode` is idempotent (safe to call when nothing is spawned)

**Acceptance**: run inside the sim container with `scenario_world` running;
spawn a face, verify it appears in `gz model --list`; set_pose, verify position
changes; despawn, verify it's gone. Background and light spawning verified visually.

---

### Step G — Episode runner ✅ DONE

**Files**: `sim/scenario_generator/episode_runner.py`

Implement `EpisodeRunner` from §9. Wire up motion patterns and bridge.

Write a smoke-test script `sim/scenario_generator/run_one_episode.py`:
```
ros2 launch ocelot sim_launch.py world:=scenario_world headless:=true use_oracle:=true &
sleep 15
python3 sim/scenario_generator/run_one_episode.py --seed 42 --duration 10
```

The script should print the scenario config, run the episode, print final face position,
and exit cleanly.

**Acceptance**: 10 episodes (different seeds) complete without error; Gazebo stays alive;
`gz model --list` is clean (no leaked entities) after each teardown.

---

### Step H — Full test suite ✅ DONE

**Files**: `tests/sim/test_scenario_generator.py`, `tests/sim/__init__.py`

Implement all tests from §11. Must run with `pytest tests/sim/` without a
running Gazebo instance (Steps A–E are pure Python; Steps F–G are mocked or skipped).

**Acceptance**: `pytest tests/sim/ -v` passes, no warnings.

---

## 14. Success Gates

- [x] `pytest tests/sim/ -v` passes (no Gazebo required)
- [x] All 6+ label keys appear in a 500-episode sample
- [x] Label distribution: no key < 5% of single-episode or multi-episode pool
- [ ] 10 sequential episodes with different seeds complete without Gazebo crash
- [ ] No entity leaks: `gz model --list` + `gz light --list` clean after each teardown
- [ ] Background texture variety confirmed visually across 5 episodes
- [ ] Lighting variation (key + fill) visible across 5 episodes (check rendered camera frames)
- [x] `ScenarioConfig.to_dict()` → JSON → `from_dict()` round-trip lossless
- [ ] `dvc status` is clean after adding backgrounds (needs Step B)
