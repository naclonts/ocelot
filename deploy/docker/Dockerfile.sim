FROM osrf/ros:jazzy-simulation

ENV DEBIAN_FRONTEND=noninteractive

# Add OSRF Gazebo stable repo (packages.osrfoundation.org).
# osrf/ros:jazzy-simulation only configures packages.ros.org; the standalone
# gz Python bindings (python3-gz-transport13, python3-gz-msgs10) live in the
# OSRF Gazebo repo and are not mirrored to the ROS repo.
RUN apt-get update && apt-get install -y --no-install-recommends curl ca-certificates \
    && curl -sSL https://packages.osrfoundation.org/gazebo.gpg \
       -o /usr/share/keyrings/pkgs-osrf-archive-keyring.gpg \
    && echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/pkgs-osrf-archive-keyring.gpg] \
       http://packages.osrfoundation.org/gazebo/ubuntu-stable noble main" \
       > /etc/apt/sources.list.d/gazebo-stable.list \
    && rm -rf /var/lib/apt/lists/*

# Gazebo bridge, ros2_control, visualization packages
RUN apt-get update && apt-get install -y --no-install-recommends \
    ros-jazzy-ros-gz \
    ros-jazzy-gz-ros2-control \
    ros-jazzy-ros2-control \
    ros-jazzy-ros2-controllers \
    ros-jazzy-joint-state-publisher \
    ros-jazzy-robot-state-publisher \
    ros-jazzy-rqt \
    ros-jazzy-rviz2 \
    # cv_bridge + python3-opencv: required by tracker_node (Step 4 parity check)
    # and by sim/generate_face_texture.py (numpy + cv2 face texture generation).
    # /usr/share/opencv4/haarcascades/ (NOT pulled in by python3-opencv itself).
    ros-jazzy-cv-bridge \
    python3-opencv \
    opencv-data \
    # gz-transport Python bindings: lets move_face.py use a persistent ZMQ
    # node instead of spawning a new gz service subprocess per pose update.
    # Eliminates 'NodeShared::RecvSrvRequest() Host unreachable' log spam.
    python3-gz-transport13 \
    python3-gz-msgs10 \
    python3-colcon-common-extensions \
    python3-pip \
    xauth \
    && rm -rf /var/lib/apt/lists/*

# Data pipeline + VLA inference dependencies
# onnxruntime-gpu: CUDA execution provider for vla_node; falls back to CPU if
# the NVIDIA runtime is not active (e.g. headless sim-only runs).
RUN pip3 install --break-system-packages h5py onnxruntime-gpu

# CUDA toolkit libraries for onnxruntime-gpu.
# The NVIDIA container runtime (compute capability) injects libcuda.so (driver
# API) but NOT the toolkit libraries: cuBLAS, cuDNN, cuFFT, etc.  onnxruntime's
# libonnxruntime_providers_cuda.so links against these at load time, so they
# must be present in the container's library path.
#
# nvidia-cudnn-cu12 pulls in nvidia-cublas-cu12 + nvidia-cuda-runtime-cu12 as
# dependencies, covering everything onnxruntime-gpu needs.  We copy the .so
# files to /usr/local/lib and run ldconfig so the dynamic linker finds them
# without any LD_LIBRARY_PATH gymnastics.
RUN pip3 install --break-system-packages \
        nvidia-cuda-runtime-cu12 \
        nvidia-cudnn-cu12 \
        nvidia-curand-cu12 \
        nvidia-cufft-cu12 \
        nvidia-cusolver-cu12 \
        nvidia-cusparse-cu12 && \
    python3 -c "import glob,os,shutil; [shutil.copy2(s,'/usr/local/lib/') for s in glob.glob('/usr/local/lib/python3.12/dist-packages/nvidia/*/lib/*.so*') if not os.path.islink(s)]" && \
    ldconfig

# Workspace setup
WORKDIR /ws
RUN mkdir -p /ws/src

# Source ROS 2 and the workspace overlay in every interactive shell.
# Hardware rendering via NVIDIA container runtime â€” no software fallback needed.
# NVIDIA_VISIBLE_DEVICES / NVIDIA_DRIVER_CAPABILITIES are set in docker-compose.sim.yml
# and cause the runtime to inject libEGL_nvidia.so / libGLX_nvidia.so at container start.
RUN echo "source /opt/ros/jazzy/setup.bash" >> /etc/bash.bashrc && \
    echo '[[ -f /ws/install/setup.bash ]] && source /ws/install/setup.bash' >> /etc/bash.bashrc

CMD ["bash"]
